{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-22T11:45:16.836367Z",
     "iopub.status.busy": "2023-05-22T11:45:16.835844Z",
     "iopub.status.idle": "2023-05-22T11:45:18.142262Z",
     "shell.execute_reply": "2023-05-22T11:45:18.140909Z",
     "shell.execute_reply.started": "2023-05-22T11:45:16.836326Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fastfood-nutrition/fastfood.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T12:29:06.146362Z",
     "iopub.status.busy": "2023-05-22T12:29:06.145833Z",
     "iopub.status.idle": "2023-05-22T12:29:06.170224Z",
     "shell.execute_reply": "2023-05-22T12:29:06.168809Z",
     "shell.execute_reply.started": "2023-05-22T12:29:06.146312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['restaurant', 'item', 'calories', 'cal_fat', 'total_fat', 'sat_fat',\n",
       "       'trans_fat', 'cholesterol', 'sodium', 'total_carb', 'fiber', 'sugar',\n",
       "       'protein', 'vit_a', 'vit_c', 'calcium', 'salad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kentucky_data=pd.read_csv('/kaggle/input/fastfood-nutrition/fastfood.csv')\n",
    "X_test_full= pd.read_csv('/kaggle/input/fastfood-nutrition/fastfood.csv')\n",
    "kentucky_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T11:45:18.183832Z",
     "iopub.status.busy": "2023-05-22T11:45:18.183419Z",
     "iopub.status.idle": "2023-05-22T11:45:18.196571Z",
     "shell.execute_reply": "2023-05-22T11:45:18.195522Z",
     "shell.execute_reply.started": "2023-05-22T11:45:18.183795Z"
    }
   },
   "outputs": [],
   "source": [
    "# target\n",
    "y = kentucky_data.restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T12:30:39.011129Z",
     "iopub.status.busy": "2023-05-22T12:30:39.010536Z",
     "iopub.status.idle": "2023-05-22T12:30:39.037532Z",
     "shell.execute_reply": "2023-05-22T12:30:39.036445Z",
     "shell.execute_reply.started": "2023-05-22T12:30:39.011077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cal_fat</th>\n",
       "      <th>total_fat</th>\n",
       "      <th>sugar</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>sodium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>95</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>410</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>130</td>\n",
       "      <td>1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>220</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>155</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>120</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cal_fat  total_fat  sugar  cholesterol  sodium\n",
       "0       60          7     11           95    1110\n",
       "1      410         45     18          130    1580\n",
       "2      600         67     18          220    1920\n",
       "3      280         31     18          155    1940\n",
       "4      410         45     18          120    1980"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating features (After completing the exercise, you can return to modify this line!)\n",
    "features = ['restaurant','cal_fat','total_fat','sugar','cholesterol','sodium']\n",
    "X = kentucky_data[features]\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "kentucky_data.dropna(axis=0, subset=['restaurant'], inplace=True)\n",
    "kentucky_data.drop(['restaurant'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "X = X.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T12:30:49.654823Z",
     "iopub.status.busy": "2023-05-22T12:30:49.654370Z",
     "iopub.status.idle": "2023-05-22T12:30:49.664283Z",
     "shell.execute_reply": "2023-05-22T12:30:49.662716Z",
     "shell.execute_reply.started": "2023-05-22T12:30:49.654787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into validation and training data\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y,train_size=0.8,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for comparing different approaches\n",
    "# really usefull function , calculating it when testing different approaches to deal with entries \n",
    "def score_dataset(train_X, valid_X, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(train_X, y_train)\n",
    "    preds = model.predict(valid_X)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T12:44:52.708899Z",
     "iopub.status.busy": "2023-05-22T12:44:52.707335Z",
     "iopub.status.idle": "2023-05-22T12:44:52.718043Z",
     "shell.execute_reply": "2023-05-22T12:44:52.716676Z",
     "shell.execute_reply.started": "2023-05-22T12:44:52.708828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 5)\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#Dealing with Missing Values : \n",
    "# Shape of training data (rows,columns)\n",
    "print(train_X.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (train_X.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although there is no Missing Value Here, just a training to deal with them ( if they were to exist):\n",
    "\n",
    "#Dropping them ALL \n",
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in train_X.columns\n",
    "                     if train_X[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_train_X = train_X.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = valid_X.drop(cols_with_missing, axis=1)\n",
    "\n",
    "# Imputation\n",
    "imputer = SimpleImputer()\n",
    "imputed_train_X = pd.DataFrame(imputer.fit_transform(train_X))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(valid_X))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_train_X.columns = train_X.columns\n",
    "imputed_X_valid.columns = valid_X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T12:37:43.243839Z",
     "iopub.status.busy": "2023-05-22T12:37:43.243416Z",
     "iopub.status.idle": "2023-05-22T12:37:43.251755Z",
     "shell.execute_reply": "2023-05-22T12:37:43.250220Z",
     "shell.execute_reply.started": "2023-05-22T12:37:43.243807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables , OBVIOUSLY none here \n",
    "s = (train_X.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T16:52:10.873635Z",
     "iopub.status.busy": "2023-05-22T16:52:10.872869Z",
     "iopub.status.idle": "2023-05-22T16:52:11.350775Z",
     "shell.execute_reply": "2023-05-22T16:52:11.349091Z",
     "shell.execute_reply.started": "2023-05-22T16:52:10.873586Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#However to train, here is the code de deal with them : Drop Categorical, Ordinal Encoding, One-hot encoding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# I - Drop :  dropping 'object' columns selected with dtypes\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m drop_train_X \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_X\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m drop_valid_X\u001b[38;5;241m=\u001b[39m valid_X\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# II - Ordinal Encoding : (fr): attribuer un id à chaque valeur\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "#However to train, here is the code de deal with them : Drop Categorical, Ordinal Encoding, One-hot encoding\n",
    "\n",
    "# I - Drop :  dropping 'object' columns selected with dtypes\n",
    "drop_train_X = train_X.select_dtypes(exclude=['object'])\n",
    "drop_valid_X= valid_X.select_dtypes(exclude=['object'])\n",
    "\n",
    "\n",
    "# II - Ordinal Encoding : (fr): attribuer un id à chaque valeur\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 1 . Make copy to avoid changing original data ??\n",
    "label_train_X = train_X.copy()\n",
    "label_valid_X = valid_X.copy()\n",
    "\n",
    "\n",
    "# Categorical columns in the training data\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "    #2.0- Probleme: si les valeurs categoriques d'entrainement et de test diffèrent, il ne sera pas possible de lier celles de validation à un id\n",
    "    # on peut donc décider de les trier entre les \"bonnes\" et les \"mauvaises\" (drop les mauvaisesà)\n",
    "\n",
    "# 2.1 Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(valid_X[col]).issubset(set(train_X[col]))]\n",
    "        \n",
    "# 2.2 Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "\n",
    "# 2.3  Drop categorical columns that will not be encoded\n",
    "label_train_X = train_X.drop(bad_label_cols, axis=1)\n",
    "label_valid_X = valid_X.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# 2.4 Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_train_X[good_label_cols] = ordinal_encoder.fit_transform(train_X[good_label_cols])\n",
    "label_valid_X[good_label_cols] = ordinal_encoder.transform(valid_X[good_label_cols])\n",
    "\n",
    "\n",
    "# III - One-hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#3.0 Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if train_X[col].nunique() < 10] # here,  select col with  less than 10 unique entries\n",
    "#3.0 Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print(low_cardinality_cols)\n",
    "\n",
    "#3.1 Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(valid_X[object_cols]))\n",
    "\n",
    "# 3.2 One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = train_X.index\n",
    "OH_cols_valid.index = valid_X.index\n",
    "\n",
    "# 3.3 Remove categorical columns (will replace with one-hot encoding)\n",
    "num_train_X = train_X.drop(object_cols, axis=1)\n",
    "num_valid_X = valid_X.drop(object_cols, axis=1)\n",
    "\n",
    "# 3.4 Add one-hot encoded columns to numerical features\n",
    "OH_train_X = pd.concat([num_train_X, OH_cols_train], axis=1)\n",
    "OH_valid_X = pd.concat([num_valid_X, OH_cols_valid], axis=1)\n",
    "\n",
    "# 3.5 Ensure all columns have the sale type, str here\n",
    "OH_train_X.columns = OH_train_X.columns.astype(str)\n",
    "OH_valid_X.columns = OH_valid_X.columns.astype(str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
