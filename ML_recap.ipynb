{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373d0985-a810-411e-a78a-8a635bc3fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce5b1a-6e4e-41fb-a85a-f455c6b3c1c6",
   "metadata": {},
   "source": [
    "Le but de ce notebook est de <b>répertorier</b> différentes fonctions ou méthodes qui vont permettre d'ajuster les paramètres de modèles ou de stratégies de nettoyage/preprocess pour avoir la MAE la plus minime possible ou tout simplement avoir des données exploitables <br>\n",
    "Source de la majorité des fonctions : <a> https://www.kaggle.com/learn </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c50e3-88d5-41f7-a6fd-d511d0be50f9",
   "metadata": {},
   "source": [
    "# 1. Fonctions pour affiner les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fddb3adc-7efc-4730-9a31-1b92308d41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf0098-aa38-460a-be78-273ae887f9db",
   "metadata": {},
   "source": [
    "## RandomForest (Regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7137064-6894-4d0d-b6c6-9dc3d74acd8d",
   "metadata": {},
   "source": [
    "### Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37200ec-9390-4c8b-bf12-8b5851ca705a",
   "metadata": {},
   "source": [
    "Parametrage des n_estimators avec un cv donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528548e-ad96-45d5-b7e2-0dd05b6b9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_RF(n_estimators, cv=5):\n",
    "    \"\"\"Return the average MAE of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    cv --- number of validation, default 5 ( équivalent répartition du train_test_split à 0.8 0.2 ) \n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps=[('imputer',SimpleImputer()),('model',RandomForestRegressor(n_estimators = n_estimators, random_state=0))])\n",
    "    scores = -1 * cross_val_score(pipeline, X, y,\n",
    "                              cv=cv,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6c3da-c157-4e89-8fd0-513da2c5df51",
   "metadata": {},
   "source": [
    "### Classique MAE pour RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebdc0d2-a1e1-4c7b-ab97-27c0fd468ff1",
   "metadata": {},
   "source": [
    "pas de cv, mae directe et brute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37c499e-d17a-4bb0-81fc-6a216d7b97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model= RandomForestRegressor(n_estimators=100, reandom_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97b033-d2cc-45a3-a17a-c72b789d2a9c",
   "metadata": {},
   "source": [
    "## XG-Boost (Regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677840f-f48f-4236-b556-50cd3bf45287",
   "metadata": {},
   "source": [
    "Calcul de RMSLE ( pas de pipeline possible (?) puisqu'on l'on utilise la méthode pandas factorize pour Ordinal encode les categoricals ) <br>\n",
    "Donc version à revoir pour automatiser ce problème ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331902a3-1d16-4c48-bed3-1c2ed213f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859f777-f4c8-4788-abd3-cdf869d16709",
   "metadata": {},
   "source": [
    "Parametrage des n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae296f68-5601-4c6e-8c16-b4c126525cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_XGBoost(n_estimator):\n",
    "    model = XGBRegressor(n_estimators=n_estimator, random_state=0)\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(preds, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd166aa-eaf7-4941-8041-a56e387465c4",
   "metadata": {},
   "source": [
    "Parametrage du learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13aa1c7-3061-4ce0-946d-d06059a4dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_XGB_lr(learning_rate):\n",
    "    model = XGBRegressor(n_estimators = 1000, learning_rate= learning_rate, random_state=0)\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(preds, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d489d5-ced8-4ed0-bb80-badb886a03c8",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c2d47-5157-464b-beee-bf561960ea4c",
   "metadata": {},
   "source": [
    "Chose bête : penser à standardiser les colonnes des features qui vont être utilisées pour du clustering avant toute chose ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c15cf-968b-453d-a453-09e3d8e4b5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b19b75-3cc4-4269-a94f-44aab279dcb2",
   "metadata": {},
   "source": [
    "# 2 Fonctions de featuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a34bbe-7b73-4704-b69a-1b0d3747dd3d",
   "metadata": {},
   "source": [
    "## 2.1 Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9700d3-58e6-4fba-af9b-66c192f7110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4188c30-60d1-4932-83ec-7761297d6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f1b8bc-0d15-474b-b41a-1df4c11b47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665dd147-cb7e-4b3d-8d08-b34ebb94898d",
   "metadata": {},
   "source": [
    "# 3 Fonctions de preprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940a7a8-7c3d-4c4f-8718-72bc60377c84",
   "metadata": {},
   "source": [
    "## 3.1 Categorical Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d7ec6-7d04-4701-8fff-6f4a5c8b6eda",
   "metadata": {},
   "source": [
    "Ces fonctions sont à lier avec une fonction de scoring pour déterminer quelle est la meilleure approche pour gérer ces différentes données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef554d71-53d0-45ec-895b-0229ff9fb6bc",
   "metadata": {},
   "source": [
    "Fonction qui pour un set de données d'entrainement donné retourne les colonnnes catégoricielles avec leur cardinal si définit comme Vrai( pour vraiment être flemmard ) sinon renvoi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356b873f-0975-4d65-82b3-d145629c2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_cols(X_train, card=False):\n",
    "        return [col for col in X_train.columns if X_train.col.dtype == 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08611d6a-2be0-4790-a6f5-c6444de21409",
   "metadata": {},
   "source": [
    "Fonction qui pour une liste de colonnes \"catégoricielles\"  et un cardinal minimum donné , retourne les colonnes à One Hot ou à Ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88378bdd-30b0-40fc-adbb-142f277f0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_cols_strategy(obj_cols, card):\n",
    "    ordinal_cols = []\n",
    "    one_hot_cols = []\n",
    "    for col in obj_cols:\n",
    "        col_card = X_train.col.nunique()\n",
    "        if col_card >= card:\n",
    "            ordinal_cols.append(col)\n",
    "        else:\n",
    "            one_hot_cols.append(col)\n",
    "    return ordinal_cols, one_hot_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4c052-a5a3-44eb-aaab-7457fe46feb5",
   "metadata": {},
   "source": [
    "pour les colonnes à OH , retourne un X_train , X_valid qui ont été One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb0428ec-6b61-4e2e-ae5f-e9232dc29fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_one_hot_encode(low_cardinality_cols):\n",
    "    # Apply one-hot encoder to each column with categorical data\n",
    "    # On pourra modifier le handle_unknown ou différents paramètres de OneHotEncoder\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = X_train.index\n",
    "    OH_cols_valid.index = X_valid.index\n",
    "\n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    num_X_train = X_train.drop(object_cols, axis=1)\n",
    "    num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "    # Ensure all columns have string type\n",
    "    OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "    OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "    return OH_X_train, OH_X_valid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86483e55-3d90-4583-a367-9153e90e316f",
   "metadata": {},
   "source": [
    "Meme principe pour un ordinal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fcde3ed-7d05-4102-a756-1b3af79b9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_ord_encode(high_cardinality_cols):\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    ord_X_train[high_cardinality_cols]=ordinal_encoder.fit_transform(X_train[high_cardinality_cols])\n",
    "    ord_X_valid[high_cardinality_cols]=ordinal_encoder.transform(X_train[high_cardinality_cols])\n",
    "    return ord_X_train, ord_X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a0378-b92a-436b-a6e4-80b61b425e17",
   "metadata": {},
   "source": [
    "Si des catégories sont présentes sur le set de validation mais pas sur celui d'entrainement, <br>\n",
    "En les \"encodant\" on peut avoir des soucis .... <br>\n",
    "La fonction suivante permet de drop celles-ci si c'est la stratégie adoptée pour les gérer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f17a9a9-ccfe-4a67-8927-0b90d94ee750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_categ_cols_withpb(obj_cols):\n",
    "    # Columns that can be safely ordinal encoded\n",
    "    good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]      \n",
    "    # Problematic columns that will be dropped from the dataset\n",
    "    bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "    \n",
    "    label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "    label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "    return label_X_train, label_X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c7f4b-a4c9-46ea-952b-1edacf323287",
   "metadata": {},
   "source": [
    "On peut aussi utiliser des \"fonctions\" pandas : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0a500-dbb0-4042-8117-9ff0da0a5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OH encode équivalent, voir doc pour arguments\n",
    "pd.get_dummies(df.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52731ade-5178-415b-993a-f8621ab7a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encode \n",
    "df[\"categorical_col\"].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb9126-faa6-43ee-9bed-d19e50c379ad",
   "metadata": {},
   "source": [
    "# 4 Fonctions de Nettoygae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcf55f-d1c8-4492-ae69-1a72a0dc287e",
   "metadata": {},
   "source": [
    "### Erreurs de Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311957c-14b3-4aad-b4fd-666fe810724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import charset_normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66de65c-0e60-4910-9c97-6ed8be674f09",
   "metadata": {},
   "source": [
    "A condition de ne pas perdre de l'information sensible, on peut les mettres toutes en minuscule et enlever les espaces inutiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0c334-bd8b-424d-a247-515f677f9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "df.col = df.col.str.lower()\n",
    "# remove trailing white spaces\n",
    "df.col = df.col.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391d5a8-3290-4c81-b962-4a90defa84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63f0a3-52cd-41b4-86ac-c0a5a44f1c82",
   "metadata": {},
   "source": [
    "### Gérer les dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a72c3-59ee-430a-b106-611aa93f7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5d04c-23b6-472d-b9df-24f95290b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_parsed'] = pd.to_datetime(df.date, format=\"%m/%d/%y\") # pour un format string de mm/dd/yy , à modifier selon le dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c61af-e162-40b6-a7be-7d009a940a49",
   "metadata": {},
   "source": [
    "Le format des entrées de dates dans une meme colonne peut etre inconsistant, on peut donc penser à vérifier les différentes longueurs de chaines de caractères présentes dans le dataset pour les repérer et écrire une fonction pour les corriger. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2a85e-4322-4ec0-80ce-47ac22cb6d75",
   "metadata": {},
   "source": [
    "### Mise à l'echelle (Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03ced8-6a28-4567-b878-df2c872ae13e",
   "metadata": {},
   "source": [
    "Changer l'echelle, Mesurer la distance entre chaque points de manière \"cohérente\"  , utile pour du SVM ou KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58caebb9-fc61-4707-a8d6-af2757595f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import minmax_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ad58d-0aec-440a-8bc1-490edfcd2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16489b-c297-4e25-aa2a-9c1f82f86a0b",
   "metadata": {},
   "source": [
    "Pour du k-means on peut calculer : (x- x.moyenne(axis=0)) / x.ecart-type(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec91ac8-8980-4065-9288-4c163f455018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sclaed est X comprenant les features utilisées pour le clustering\n",
    "X_scaled = (X_scaled - X_scaled.means(axis=0)) / X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09368e-706b-4db4-8125-71e59b830949",
   "metadata": {},
   "source": [
    "### Normalisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917289e4-07fb-41e4-9ff5-c44d3300c6c5",
   "metadata": {},
   "source": [
    "Changer la forme de la distribution de ses données pour l'exploiter via du Gaussian Naive Bayes ou LDA ( Linear Discriminant Analysis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091951eb-8492-464c-8546-8c7204051c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164bd3e3-34fd-4faf-ae19-bc4e0bad4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = stats.boxcox(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec28df-d955-4047-8641-08a842300d3c",
   "metadata": {},
   "source": [
    "### Données nulles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81afc356-b2ad-4f27-b558-e97215819f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ebdb3-bcda-4ad5-b24d-eaa44e580641",
   "metadata": {},
   "source": [
    "ou le classique pandas : .fillna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab1426-2fa2-4ab5-bf52-fb39aab6789d",
   "metadata": {},
   "source": [
    "# Annexe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32119561-12e7-440e-9273-362c3f42fbc7",
   "metadata": {},
   "source": [
    "##  Déterminer l'encodage d'un fichier pour le lire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "302a6b8d-92ce-4295-be58-295fed898a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import charset_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10ec3b-008e-4880-aabf-8d94b891c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"path/your_file.csv\", 'rb') as rawdata:\n",
    "    result = charset_normalizer.detect(rawdata.read())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71696f85-3ae0-4f8a-895d-fa5086eb8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"path/your_file.csv\", encoding='enter_the_encoding_result_here')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
