{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-22T11:45:16.835844Z","iopub.execute_input":"2023-05-22T11:45:16.836367Z","iopub.status.idle":"2023-05-22T11:45:18.142262Z","shell.execute_reply.started":"2023-05-22T11:45:16.836326Z","shell.execute_reply":"2023-05-22T11:45:18.140909Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/fastfood-nutrition/fastfood.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"kentucky_data=pd.read_csv('/kaggle/input/fastfood-nutrition/fastfood.csv')\nX_test_full= pd.read_csv('/kaggle/input/fastfood-nutrition/fastfood.csv')\nkentucky_data.columns","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:29:06.145833Z","iopub.execute_input":"2023-05-22T12:29:06.146362Z","iopub.status.idle":"2023-05-22T12:29:06.170224Z","shell.execute_reply.started":"2023-05-22T12:29:06.146312Z","shell.execute_reply":"2023-05-22T12:29:06.168809Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Index(['restaurant', 'item', 'calories', 'cal_fat', 'total_fat', 'sat_fat',\n       'trans_fat', 'cholesterol', 'sodium', 'total_carb', 'fiber', 'sugar',\n       'protein', 'vit_a', 'vit_c', 'calcium', 'salad'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"# target\ny = kentucky_data.restaurant","metadata":{"execution":{"iopub.status.busy":"2023-05-22T11:45:18.183419Z","iopub.execute_input":"2023-05-22T11:45:18.183832Z","iopub.status.idle":"2023-05-22T11:45:18.196571Z","shell.execute_reply.started":"2023-05-22T11:45:18.183795Z","shell.execute_reply":"2023-05-22T11:45:18.195522Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Creating features (After completing the exercise, you can return to modify this line!)\nfeatures = ['restaurant','cal_fat','total_fat','sugar','cholesterol','sodium']\nX = kentucky_data[features]\n# Remove rows with missing target, separate target from predictors\nkentucky_data.dropna(axis=0, subset=['restaurant'], inplace=True)\nkentucky_data.drop(['restaurant'], axis=1, inplace=True)\n\n# To keep things simple, we'll use only numerical predictors\nX = X.select_dtypes(exclude=['object'])\nX_test = X_test_full.select_dtypes(exclude=['object'])\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:30:39.010536Z","iopub.execute_input":"2023-05-22T12:30:39.011129Z","iopub.status.idle":"2023-05-22T12:30:39.037532Z","shell.execute_reply.started":"2023-05-22T12:30:39.011077Z","shell.execute_reply":"2023-05-22T12:30:39.036445Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   cal_fat  total_fat  sugar  cholesterol  sodium\n0       60          7     11           95    1110\n1      410         45     18          130    1580\n2      600         67     18          220    1920\n3      280         31     18          155    1940\n4      410         45     18          120    1980","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cal_fat</th>\n      <th>total_fat</th>\n      <th>sugar</th>\n      <th>cholesterol</th>\n      <th>sodium</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>60</td>\n      <td>7</td>\n      <td>11</td>\n      <td>95</td>\n      <td>1110</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>410</td>\n      <td>45</td>\n      <td>18</td>\n      <td>130</td>\n      <td>1580</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>600</td>\n      <td>67</td>\n      <td>18</td>\n      <td>220</td>\n      <td>1920</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>280</td>\n      <td>31</td>\n      <td>18</td>\n      <td>155</td>\n      <td>1940</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>410</td>\n      <td>45</td>\n      <td>18</td>\n      <td>120</td>\n      <td>1980</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Split into validation and training data\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y,train_size=0.8,test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:30:49.654370Z","iopub.execute_input":"2023-05-22T12:30:49.654823Z","iopub.status.idle":"2023-05-22T12:30:49.664283Z","shell.execute_reply.started":"2023-05-22T12:30:49.654787Z","shell.execute_reply":"2023-05-22T12:30:49.662716Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Function for comparing different approaches\n# really usefull function , calculating it when testing different approaches to deal with entries \ndef score_dataset(train_X, valid_X, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(train_X, y_train)\n    preds = model.predict(valid_X)\n    return mean_absolute_error(y_valid, preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dealing with Missing Values : \n# Shape of training data (rows,columns)\nprint(train_X.shape)\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (train_X.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:44:52.707335Z","iopub.execute_input":"2023-05-22T12:44:52.708899Z","iopub.status.idle":"2023-05-22T12:44:52.718043Z","shell.execute_reply.started":"2023-05-22T12:44:52.708828Z","shell.execute_reply":"2023-05-22T12:44:52.716676Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"(412, 5)\nSeries([], dtype: int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Although there is no Missing Value Here, just a training to deal with them ( if they were to exist):\n\n#Dropping them ALL \n# Get names of columns with missing values\ncols_with_missing = [col for col in train_X.columns\n                     if train_X[col].isnull().any()]\n\n# Drop columns in training and validation data\nreduced_train_X = train_X.drop(cols_with_missing, axis=1)\nreduced_X_valid = valid_X.drop(cols_with_missing, axis=1)\n\n# Imputation\nimputer = SimpleImputer()\nimputed_train_X = pd.DataFrame(imputer.fit_transform(train_X))\nimputed_X_valid = pd.DataFrame(imputer.transform(valid_X))\n\n# Imputation removed column names; put them back\nimputed_train_X.columns = train_X.columns\nimputed_X_valid.columns = valid_X.columns\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get list of categorical variables , OBVIOUSLY none here \ns = (train_X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:37:43.243416Z","iopub.execute_input":"2023-05-22T12:37:43.243839Z","iopub.status.idle":"2023-05-22T12:37:43.251755Z","shell.execute_reply.started":"2023-05-22T12:37:43.243807Z","shell.execute_reply":"2023-05-22T12:37:43.250220Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Categorical variables:\n[]\n","output_type":"stream"}]},{"cell_type":"code","source":"#However to train, here is the code de deal with them : Drop Categorical, Ordinal Encoding, One-hot encoding\n\n# I - Drop :  dropping 'object' columns selected with dtypes\ndrop_train_X = train_X.select_dtypes(exclude=['object'])\ndrop_valid_X= valid_X.select_dtypes(exclude=['object'])\n\n\n# II - Ordinal Encoding : (fr): attribuer un id à chaque valeur\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# 1 . Make copy to avoid changing original data ??\nlabel_train_X = train_X.copy()\nlabel_valid_X = valid_X.copy()\n\n\n# Categorical columns in the training data\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n    #2.0- Probleme: si les valeurs categoriques d'entrainement et de test diffèrent, il ne sera pas possible de lier celles de validation à un id\n    # on peut donc décider de les trier entre les \"bonnes\" et les \"mauvaises\" (drop les mauvaisesà)\n\n# 2.1 Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(valid_X[col]).issubset(set(train_X[col]))]\n        \n# 2.2 Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\n# 2.3  Drop categorical columns that will not be encoded\nlabel_train_X = train_X.drop(bad_label_cols, axis=1)\nlabel_valid_X = valid_X.drop(bad_label_cols, axis=1)\n\n# 2.4 Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabel_train_X[good_label_cols] = ordinal_encoder.fit_transform(train_X[good_label_cols])\nlabel_valid_X[good_label_cols] = ordinal_encoder.transform(valid_X[good_label_cols])\n\n\n# III - One-hot encoding\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n#3.0 Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if train_X[col].nunique() < 10] # here,  select col with  less than 10 unique entries\n#3.0 Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint(low_cardinality_cols)\n\n#3.1 Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(valid_X[object_cols]))\n\n# 3.2 One-hot encoding removed index; put it back\nOH_cols_train.index = train_X.index\nOH_cols_valid.index = valid_X.index\n\n# 3.3 Remove categorical columns (will replace with one-hot encoding)\nnum_train_X = train_X.drop(object_cols, axis=1)\nnum_valid_X = valid_X.drop(object_cols, axis=1)\n\n# 3.4 Add one-hot encoded columns to numerical features\nOH_train_X = pd.concat([num_train_X, OH_cols_train], axis=1)\nOH_valid_X = pd.concat([num_valid_X, OH_cols_valid], axis=1)\n\n# 3.5 Ensure all columns have the sale type, str here\nOH_train_X.columns = OH_train_X.columns.astype(str)\nOH_valid_X.columns = OH_valid_X.columns.astype(str)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:52:10.872869Z","iopub.execute_input":"2023-05-22T16:52:10.873635Z","iopub.status.idle":"2023-05-22T16:52:11.350775Z","shell.execute_reply.started":"2023-05-22T16:52:10.873586Z","shell.execute_reply":"2023-05-22T16:52:11.349091Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#However to train, here is the code de deal with them : Drop Categorical, Ordinal Encoding, One-hot encoding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# I - Drop :  dropping 'object' columns selected with dtypes\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m drop_train_X \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_X\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m drop_valid_X\u001b[38;5;241m=\u001b[39m valid_X\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# II - Ordinal Encoding : (fr): attribuer un id à chaque valeur\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"],"ename":"NameError","evalue":"name 'train_X' is not defined","output_type":"error"}]}]}